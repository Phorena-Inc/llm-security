#!/usr/bin/env python3
"""
Prompt Sensitivity / Output Policy Example using Graphiti

This example demonstrates how to implement content filtering and redaction policies:
- Phase-specific content redaction
- Firewall detection of sensitive terms
- Dynamic redaction based on mission phase
- Graphiti knowledge graph integration for policy storage

Policy Rules:
- Code names must be redacted during Active Mission phase
- Geographic locations must be protected during Active Mission
- Personnel identities must be hidden during Active Mission and Post Mission
- Emergency phase allows full disclosure for safety
"""

import os
import asyncio
import json
import logging
import re
from datetime import datetime, timezone
from typing import Dict, Any, Optional, List, Tuple
from enum import Enum

from graphiti_core import Graphiti
from graphiti_core.llm_client import LLMConfig
from graphiti_core.llm_client.groq_client import GroqClient
from graphiti_core.nodes import EpisodeType


def load_policy_config(config_file: str = "prompt_sensitivity_config.json") -> Dict[str, Any]:
    """Load policy configuration from JSON file"""
    try:
        with open(config_file, 'r') as f:
            config = json.load(f)
        return config
    except FileNotFoundError:
        print(f"Warning: {config_file} not found, using default configuration")
        return {
            "sensitive_terms": {
                "codename": {
                    "description": "Code names and project identifiers",
                    "redaction_pattern": "[REDACTED]",
                    "examples": ["Project Zeus", "Alpha Protocol"]
                }
            },
            "mission_phases": {
                "active_mission": {
                    "description": "Active mission execution phase",
                    "redacted_terms": ["codename", "location", "personnel"]
                }
            },
            "test_scenarios": [
                {
                    "phase": "active_mission",
                    "input_text": "Project Zeus is moving to phase 3.",
                    "expected_output": "[REDACTED] is moving to phase 3.",
                    "description": "Active mission with code name redaction"
                }
            ],
            "redaction_rules": [
                "Code names must be redacted during Active Mission phase"
            ]
        }
    except json.JSONDecodeError as e:
        print(f"Error parsing {config_file}: {e}")
        return {}


def setup_logging() -> logging.Logger:
    """Setup logging configuration with timestamps and unique log file per run"""
    logger = logging.getLogger('prompt_sensitivity_policy')
    logger.setLevel(logging.DEBUG)
    logger.handlers.clear()
    now = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_filename = f'prompt_sensitivity_policy_{now}.log'
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    file_handler = logging.FileHandler(log_filename, mode='w')
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    return logger


class PromptSensitivityPolicy:
    """Implements prompt sensitivity and output redaction policies using Graphiti"""
    
    def __init__(self, graphiti: Graphiti, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        self.graphiti = graphiti
        self.config = config
        self.policies_added = False
        self.logger = logger or logging.getLogger('prompt_sensitivity_policy')
        
        # Build sensitive terms dictionary for quick lookup
        self.sensitive_terms = {}
        for term_type, term_config in config.get("sensitive_terms", {}).items():
            for example in term_config.get("examples", []):
                self.sensitive_terms[example.lower()] = {
                    "type": term_type,
                    "pattern": term_config.get("redaction_pattern", "[REDACTED]"),
                    "sensitivity": term_config.get("sensitivity_level", "medium")
                }
    
    async def add_policies(self):
        """Add prompt sensitivity policies to the knowledge graph"""
        try:
            if self.policies_added:
                self.logger.info("Prompt sensitivity policies already added to graph - skipping")
                return
            
            self.logger.info("Starting to add prompt sensitivity policies to knowledge graph...")
            
            # Load policies from JSON configuration and filter out problematic ones
            all_policies = self.config.get("redaction_rules", [
                "Code names must be redacted during Active Mission phase"
            ])
            
            # Filter out policies that might cause Neo4j property issues
            policies = []
            for policy in all_policies:
                # Skip policies that contain complex structures or might be parsed as objects
                if any(keyword in policy.lower() for keyword in ['timestamp', 'timezone', 'status', 'pattern', 'preservation', 'restriction']):
                    self.logger.warning(f"Skipping potentially problematic policy: {policy[:50]}...")
                    continue
                policies.append(policy)
            
            self.logger.debug(f"Preparing to add {len(policies)} text policies to graph")
            
            # Add text-based policies with minimal entity extraction
            for i, policy in enumerate(policies):
                try:
                    self.logger.debug(f"Adding text policy {i+1}/{len(policies)}")
                    
                    # Check for non-ASCII characters
                    non_ascii_chars = [c for c in policy if ord(c) > 127]
                    if non_ascii_chars:
                        self.logger.warning(f"Non-ASCII characters found in text policy {i+1}: {[repr(c) for c in non_ascii_chars]}")
                    
                    # Add episode with empty entity_types to prevent automatic entity extraction
                    await self.graphiti.add_episode(
                        name=f"Prompt Sensitivity Policy - Text {i+1}",
                        episode_body=policy,
                        source=EpisodeType.text,
                        source_description="Prompt sensitivity policy rule",
                        reference_time=datetime.now(timezone.utc),
                        entity_types={}  # Empty dict prevents automatic entity extraction
                    )
                    self.logger.debug(f"Successfully added text policy {i+1}")
                    
                except Exception as e:
                    self.logger.error(f"Failed to add text policy {i+1}: {str(e)}")
                    # Continue with other policies instead of failing completely
                    continue
            
            self.policies_added = True
            self.logger.info("Successfully added all prompt sensitivity policies to knowledge graph")
            
        except Exception as e:
            self.logger.error(f"Error in add_policies: {str(e)}")
            # Mark as added to prevent retry attempts
            self.policies_added = True
            self.logger.info("Policy addition completed with some errors")
    
    def detect_sensitive_terms(self, text: str) -> List[Dict[str, Any]]:
        """Detect sensitive terms in the given text"""
        detected_terms = []
        
        try:
            text_lower = text.lower()
            
            for term, config in self.sensitive_terms.items():
                if term.lower() in text_lower:
                    # Find all occurrences
                    pattern = re.compile(re.escape(term), re.IGNORECASE)
                    matches = pattern.finditer(text)
                    
                    for match in matches:
                        detected_terms.append({
                            "term": match.group(),
                            "type": config["type"],
                            "pattern": config["pattern"],
                            "start": match.start(),
                            "end": match.end(),
                            "sensitivity": config["sensitivity"]
                        })
            
            self.logger.debug(f"Detected {len(detected_terms)} sensitive terms in text")
            return detected_terms
            
        except Exception as e:
            self.logger.error(f"Error detecting sensitive terms: {str(e)}")
            return []
    
    def should_redact_term(self, term_type: str, mission_phase: str) -> bool:
        """Determine if a term should be redacted based on mission phase"""
        try:
            phase_config = self.config.get("mission_phases", {}).get(mission_phase.lower())
            if not phase_config:
                self.logger.warning(f"Unknown mission phase: {mission_phase}")
                return False
            
            redacted_terms = phase_config.get("redacted_terms", [])
            return term_type in redacted_terms
            
        except Exception as e:
            self.logger.error(f"Error checking redaction rules: {str(e)}")
            return False
    
    def redact_text(self, text: str, mission_phase: str) -> Tuple[str, List[Dict[str, Any]]]:
        """
        Redact sensitive terms from text based on mission phase
        
        Args:
            text: Input text to redact
            mission_phase: Current mission phase
        
        Returns:
            Tuple of (redacted_text, redaction_log)
        """
        try:
            self.logger.info(f"Redacting text for mission phase: {mission_phase}")
            
            # Detect sensitive terms
            detected_terms = self.detect_sensitive_terms(text)
            redaction_log = []
            redacted_text = text
            
            # Sort by position (reverse order to avoid index shifting)
            detected_terms.sort(key=lambda x: x["start"], reverse=True)
            
            for term_info in detected_terms:
                term_type = term_info["type"]
                
                # Check if this term should be redacted in current phase
                if self.should_redact_term(term_type, mission_phase):
                    # Perform redaction
                    original_term = term_info["term"]
                    redaction_pattern = term_info["pattern"]
                    
                    # Replace the term
                    redacted_text = (
                        redacted_text[:term_info["start"]] + 
                        redaction_pattern + 
                        redacted_text[term_info["end"]:]
                    )
                    
                    redaction_log.append({
                        "original": original_term,
                        "redacted": redaction_pattern,
                        "type": term_type,
                        "phase": mission_phase,
                        "position": term_info["start"]
                    })
                    
                    self.logger.info(f"Redacted '{original_term}' to '{redaction_pattern}' (type: {term_type})")
                else:
                    self.logger.debug(f"Term '{term_info['term']}' (type: {term_type}) not redacted in phase {mission_phase}")
            
            return redacted_text, redaction_log
            
        except Exception as e:
            self.logger.error(f"Error redacting text: {str(e)}")
            return text, []
    
    async def check_policy(self, input_text: str, mission_phase: str) -> Dict[str, Any]:
        """
        Check and apply prompt sensitivity policy
        
        Args:
            input_text: Text to check and potentially redact
            mission_phase: Current mission phase
        
        Returns:
            Dict with policy decision and redacted text
        """
        try:
            self.logger.info(f"Checking prompt sensitivity policy for phase: {mission_phase}")
            
            # Validate inputs
            if not input_text or not isinstance(input_text, str):
                self.logger.error(f"Invalid input_text provided: {input_text}")
                return {
                    "success": False,
                    "error": "Input text must be a non-empty string",
                    "original_text": input_text,
                    "redacted_text": input_text,
                    "mission_phase": mission_phase
                }
            
            if not mission_phase or not isinstance(mission_phase, str):
                self.logger.error(f"Invalid mission_phase provided: {mission_phase}")
                return {
                    "success": False,
                    "error": "Mission phase must be a non-empty string",
                    "original_text": input_text,
                    "redacted_text": input_text,
                    "mission_phase": mission_phase
                }
            
            # Detect sensitive terms
            detected_terms = self.detect_sensitive_terms(input_text)
            
            # Apply redaction
            redacted_text, redaction_log = self.redact_text(input_text, mission_phase)
            
            # Query the knowledge graph for relevant policies (optimized)
            search_query = f"prompt sensitivity policy {mission_phase}"
            self.logger.debug(f"Searching graph with optimized query: {search_query}")
            
            try:
                results = await self.graphiti.search(search_query)
                self.logger.debug(f"Graph search returned {len(results) if results else 0} results")
            except Exception as e:
                self.logger.error(f"Error during graph search: {str(e)}")
                # Continue without graph results - policy logic will still work
                results = []
                self.logger.info("Continuing with policy logic despite graph search error")
            
            return {
                "success": True,
                "original_text": input_text,
                "redacted_text": redacted_text,
                "mission_phase": mission_phase,
                "detected_terms": detected_terms,
                "redaction_log": redaction_log,
                "redaction_applied": len(redaction_log) > 0,
                "policy_applied": "prompt_sensitivity_redaction"
            }
            
        except Exception as e:
            self.logger.error(f"Unexpected error in check_policy: {str(e)}")
            return {
                "success": False,
                "error": f"Unexpected error: {str(e)}",
                "original_text": input_text,
                "redacted_text": input_text,
                "mission_phase": mission_phase
            }


def print_graphiti_representation(input_text: str, mission_phase: str, result: dict, description: str = None):
    """Print a dynamic Graphiti Representation for the redaction situation."""
    try:
        if description:
            print(f"\nGraphiti Representation for: {description}")
            print(f"   Test: Redacting text during {mission_phase} phase")
        else:
            print("\nGraphiti Representation:")
        
        # Show the input text relationship
        print(f"InputText ‚Äî[CONTAINS]‚Üí SensitiveTerms")
        
        # Show detected terms and their redaction status
        detected_terms = result.get("detected_terms", [])
        redaction_log = result.get("redaction_log", [])
        
        for term_info in detected_terms:
            term_type = term_info["type"].title()
            term_name = term_info["term"]
            
            # Check if this term was redacted
            was_redacted = any(r["original"] == term_name for r in redaction_log)
            
            if was_redacted:
                print(f"{term_name} ‚Äî[TAGGED_AS]‚Üí {term_type}")
                print(f"{term_type} ‚Äî[REDACT_OUTPUT_IN]‚Üí {mission_phase.title()}")
                print(f"Redaction: {term_name} ‚Üí {term_info['pattern']}")
            else:
                print(f"{term_name} ‚Äî[TAGGED_AS]‚Üí {term_type}")
                print(f"{term_type} ‚Äî[ALLOWED_IN]‚Üí {mission_phase.title()}")
                print(f"Preserved: {term_name} (allowed in {mission_phase} phase)")
        
        # Show the current phase
        phase_display = {
            "pre_deployment": "Pre-Deployment",
            "active_mission": "Active Mission", 
            "post_mission": "Post Mission",
            "emergency": "Emergency"
        }
        current_phase = phase_display.get(mission_phase.lower(), mission_phase.title())
        print(f"CurrentPhase = {current_phase}")
        
        # Show the redaction result
        redaction_applied = result.get("redaction_applied", False)
        if redaction_applied:
            print(f"Resolution: Text redacted for {mission_phase} phase")
        else:
            print(f"Resolution: No redaction needed for {mission_phase} phase")
        
    except Exception as e:
        print(f"Error printing Graphiti Representation: {str(e)}")


async def main():
    """Main function to demonstrate the prompt sensitivity policy"""
    
    # Setup logging
    logger = setup_logging()
    logger.info("Starting Prompt Sensitivity Policy demonstration")
    
    # Check for required environment variables
    if not os.getenv("GROQ_API_KEY"):
        logger.error("GROQ_API_KEY environment variable is required")
        print("‚ùå ERROR: GROQ_API_KEY environment variable is required")
        return
    
    # Initialize Graphiti
    logger.info("Initializing Graphiti...")
    
    # Get Neo4j connection details from environment or use defaults
    neo4j_uri = os.getenv("NEO4J_URI", "bolt://localhost:7687")
    neo4j_user = os.getenv("NEO4J_USER", "neo4j")
    neo4j_password = os.getenv("NEO4J_PASSWORD", "password")
    
    logger.info(f"Connecting to Neo4j at {neo4j_uri} with user {neo4j_user}")
    
    try:
        # Initialize Groq LLM client
        logger.debug("Initializing Groq LLM client")
        llm_config = LLMConfig(
            api_key=os.getenv("GROQ_API_KEY"),
            model="llama-3.3-70b-versatile"
        )
        llm_client = GroqClient(llm_config)
        logger.debug("Groq LLM client initialized successfully")
        
        # Initialize Graphiti with the LLM client
        logger.debug("Initializing Graphiti instance")
        graphiti = Graphiti(neo4j_uri, neo4j_user, neo4j_password, llm_client)
        logger.info("Graphiti instance created successfully")
        
        # Build indices and constraints
        logger.info("Building indices and constraints...")
        await graphiti.build_indices_and_constraints()
        logger.info("Indices and constraints built successfully")
        
        # Load policy configuration from JSON
        logger.info("Loading policy configuration...")
        config = load_policy_config()
        logger.info("Policy configuration loaded successfully")
        
        # Initialize the policy checker with configuration
        policy_checker = PromptSensitivityPolicy(graphiti, config, logger)
        logger.info("PromptSensitivityPolicy instance created successfully")
        
        # Add policies to the knowledge graph
        logger.info("Adding policies to knowledge graph...")
        await policy_checker.add_policies()
        
        # Test the policy with different scenarios
        logger.info("Starting policy tests...")
        print("\n" + "="*80)
        print("üîê Testing Prompt Sensitivity / Output Policy")
        print("="*80)
        
        # Load test scenarios from configuration
        test_scenarios = config.get("test_scenarios", [])
        
        # Fallback to default scenarios if none in config
        if not test_scenarios:
            test_scenarios = [
                {
                    "phase": "active_mission",
                    "input_text": "Project Zeus is moving to phase 3.",
                    "expected_output": "[REDACTED] is moving to phase 3.",
                    "description": "Active mission with code name redaction"
                }
            ]
        
        for scenario in test_scenarios:
            phase = scenario["phase"]
            input_text = scenario["input_text"]
            expected_output = scenario["expected_output"]
            description = scenario["description"]
            
            logger.info(f"Running test: {description}")
            print(f"\nüìã Test: {description}")
            
            try:
                result = await policy_checker.check_policy(input_text, phase)
                
                # Print the dynamic Graphiti Representation
                print_graphiti_representation(input_text, phase, result, description)
                
                if result["success"]:
                    print(f"   Original: {result['original_text']}")
                    print(f"   Redacted: {result['redacted_text']}")
                    
                    # Check if redaction was applied
                    if result.get("redaction_applied"):
                        print(f"   ‚úÖ REDACTION APPLIED")
                        redaction_log = result.get("redaction_log", [])
                        for redaction in redaction_log:
                            print(f"      {redaction['original']} ‚Üí {redaction['redacted']} ({redaction['type']})")
                    else:
                        print(f"   ‚ÑπÔ∏è  NO REDACTION NEEDED")
                    
                    # Check if output matches expected
                    if result["redacted_text"] == expected_output:
                        print(f"   ‚úÖ EXPECTED OUTPUT MATCH")
                    else:
                        print(f"   ‚ö†Ô∏è  OUTPUT MISMATCH")
                        print(f"      Expected: {expected_output}")
                        print(f"      Got: {result['redacted_text']}")
                    
                    logger.info(f"Test result: Success - {len(result.get('redaction_log', []))} terms redacted")
                else:
                    print(f"   ‚ùå ERROR: {result.get('error', 'Unknown error')}")
                    logger.error(f"Test failed: {result.get('error', 'Unknown error')}")
                
            except Exception as e:
                error_msg = f"Error during test '{description}': {str(e)}"
                logger.error(error_msg)
                print(f"   ERROR: {error_msg}")
        
        # Test the specific scenario from the prompt
        logger.info("Running original scenario test...")
        print(f"\nüéØ Test: Original Scenario - Project Zeus during Active Mission")
        
        try:
            original_result = await policy_checker.check_policy(
                "Project Zeus is moving to phase 3.",
                "active_mission"
            )
            print_graphiti_representation("Project Zeus is moving to phase 3.", "active_mission", original_result, "Original Scenario - Project Zeus during Active Mission")
            
            if original_result["success"]:
                print(f"   Original: {original_result['original_text']}")
                print(f"   Redacted: {original_result['redacted_text']}")
                print(f"   Firewall detected 'Project Zeus' ‚Üí Graphiti shows it's a CodeName")
                print(f"   Finds rule: redact output ‚Üí rewrites to: '{original_result['redacted_text']}'")
                logger.info(f"Original scenario result: Success - {len(original_result.get('redaction_log', []))} terms redacted")
            else:
                print(f"   ‚ùå ERROR: {original_result.get('error', 'Unknown error')}")
                
        except Exception as e:
            error_msg = f"Error during original scenario test: {str(e)}"
            logger.error(error_msg)
            print(f"   ERROR: {error_msg}")
        
        print("\n" + "="*80)
        print("üéâ Prompt Sensitivity Policy demonstration completed!")
        print("="*80)
        logger.info("Prompt Sensitivity Policy demonstration completed successfully")
        
    except Exception as e:
        error_msg = f"Critical error in main function: {str(e)}"
        logger.error(error_msg)
        print(f"‚ùå ERROR: {error_msg}")
        print("\nMake sure:")
        print("1. Neo4j is running and accessible")
        print("2. GROQ_API_KEY is set in your environment")
        print("3. Neo4j credentials are correct")
        print("4. Check the log file 'prompt_sensitivity_policy.log' for detailed error information")
        
    finally:
        # Close the connection
        if 'graphiti' in locals():
            try:
                logger.info("Closing Graphiti connection...")
                await graphiti.close()
                logger.info("Graphiti connection closed successfully")
            except Exception as e:
                logger.error(f"Error closing Graphiti connection: {str(e)}")
        
        logger.info("Prompt Sensitivity Policy demonstration finished")


if __name__ == "__main__":
    asyncio.run(main()) 